{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPsRfkpx1ovRii/KIBTCjLH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shimmer0523/voice-diarizer/blob/main/voice_diarizer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyannote.audio pydub noisereduce"
      ],
      "metadata": {
        "id": "ErE6jpL5d93u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55hPmTDacp6n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pyannote.audio import Pipeline\n",
        "from pyannote.audio.pipelines.utils.hook import ProgressHook\n",
        "from pyannote.core import Segment, Annotation\n",
        "from transformers import AutoFeatureExtractor, AutoModelForAudioXVector\n",
        "import numpy as np\n",
        "from pydub import AudioSegment\n",
        "import noisereduce as nr\n",
        "import soundfile as sf\n",
        "from moviepy.editor import VideoFileClip\n",
        "import torch\n",
        "import torchaudio\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def mp4_to_audio(mp4_file):\n",
        "    \"\"\"mp4 to audio\n",
        "\n",
        "    Args:\n",
        "        mp4_file (string): file path of mp4 file\n",
        "        output_audio (number[]): extracted audio data\n",
        "    \"\"\"\n",
        "    video = VideoFileClip(mp4_file)\n",
        "    audio = video.audio\n",
        "    file_body = os.path.splitext(os.path.basename(mp4_file))[0]\n",
        "    audio_file = \"audio_\" + file_body + \".wav\"\n",
        "    print(\"export: \" + audio_file)\n",
        "    audio.write_audiofile(audio_file, codec=\"pcm_s16le\")\n",
        "    audio.close()\n",
        "    video.close()\n",
        "\n",
        "    return audio_file\n",
        "\n",
        "\n",
        "class Section:\n",
        "    def __init__(self, section_start, section_end, noise_start, noise_end) -> None:\n",
        "        self.section_start = section_start\n",
        "        self.section_end = section_end\n",
        "        self.noise_start = noise_start\n",
        "        self.noise_end = noise_end\n",
        "\n",
        "\n",
        "def reduce_noise(waveform: np.ndarray, sample_rate, sections: list[Section]) -> torch.Tensor:\n",
        "    output = waveform\n",
        "\n",
        "    for s in sections:\n",
        "        y = waveform[s.section_start * sample_rate : s.section_end * sample_rate]\n",
        "        y_noise = waveform[s.noise_start * sample_rate : s.noise_end * sample_rate]\n",
        "\n",
        "        output = np.concatenate((\n",
        "            waveform[: s.section_start],\n",
        "            nr.reduce_noise(y=y, y_noise=y_noise, sr=sample_rate, stationary=True),\n",
        "            waveform[s.section_end :]\n",
        "        ))\n",
        "    return torch.from_numpy(output)\n"
      ],
      "metadata": {
        "id": "f_YblBrKeVtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODE = \"AUDIO\" # AUDIO or MP4\n",
        "AUDIO_FILE = \"g22.wav\"\n",
        "SECTIONS = [Section(section_start=0, section_end=-0.1, noise_start=0, noise_end=0.1)]"
      ],
      "metadata": {
        "id": "0aXDt46xkHDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MP4_FILE = \"bb02.mb4\"\n",
        "# SECTIONS = [[\n",
        "#         Section(section_start=0, section_end=35, noise_start=1, noise_end=3),\n",
        "#         Section(section_start=35, section_end=-1, noise_start=69, noise_end=70),\n",
        "#     ]]\n"
      ],
      "metadata": {
        "id": "gWgFJU8SAnFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline = Pipeline.from_pretrained(\n",
        "    \"pyannote/speaker-diarization-3.1\", use_auth_token=userdata.get('HUGGINGFACE_TOKEN')\n",
        ")\n",
        "pipeline.to(torch.device(\"cuda\"))\n",
        "\n",
        "if MODE == \"MP4\":\n",
        "    audio_file = mp4_to_audio(MP4_FILE)\n",
        "else:\n",
        "    audio_file = AUDIO_FILE\n",
        "\n",
        "audio = AudioSegment.from_wav(audio_file).set_channels(1)\n",
        "waveform = np.array(audio.get_array_of_samples())\n",
        "sampling_rate = audio.frame_rate\n",
        "\n",
        "denoised_waveform = reduce_noise(\n",
        "    waveform,\n",
        "    sampling_rate,\n",
        "    sections=SECTIONS,\n",
        ")\n",
        "\n",
        "with ProgressHook() as hook:\n",
        "    diarization: Annotation = pipeline(\n",
        "        {\"waveform\": denoised_waveform, \"sample_rate\": sampling_rate}, num_speakers=2, hook=hook\n",
        "    )\n",
        "\n",
        "for segment, track_name, label in diarization.itertracks(yield_label=True):\n",
        "    print(f\"{segment.start=:.1f}, {segment.end=:.1f}, {track_name=}, {label=}\")\n",
        "\n",
        "remaining_speakers = [\"SPEAKER_00\", \"SPEAKER_01\"]\n",
        "\n",
        "for speaker in remaining_speakers:\n",
        "    print(\"open: \" + audio_file)\n",
        "    audio = AudioSegment.from_wav(audio_file)\n",
        "\n",
        "    for segment, track_name, label in diarization.itertracks(yield_label=True):\n",
        "        if label != speaker:\n",
        "            mute_start = segment.start * 1000\n",
        "            mute_end = segment.end * 1000\n",
        "            muted_section = AudioSegment.silent(duration=(mute_end - mute_start), frame_rate=sampling_rate)\n",
        "            audio = audio[:mute_start] + muted_section + audio[mute_end:]\n",
        "            print(f\"mute: {mute_start} - {mute_end}\")\n",
        "\n",
        "    export_file = \"muted_\" + speaker + \"_\" + os.path.basename(audio_file)\n",
        "\n",
        "    print(\"export: \" + export_file)\n",
        "    audio.export(export_file, format=\"wav\")\n"
      ],
      "metadata": {
        "id": "FtqEKz5wecm9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}